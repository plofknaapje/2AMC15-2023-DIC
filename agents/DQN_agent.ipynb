{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is based on and partly copied from these two sources: \n",
    "- https://arshren.medium.com/deep-q-learning-a-deep-reinforcement-learning-algorithm-f1366cf1b53d\n",
    "- https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "NOTE: Do not forget to change this notebook into a .py file before submission. Also, save and load the model after it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "# import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\dough\\\\Documents\\\\DIC\\\\2AMC15-2023-DIC'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "new_path = cwd[:-7]\n",
    "new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('c:\\\\Users\\\\dough\\\\Documents\\\\DIC\\\\2AMC15-2023-DIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fix the imports below, since they are required for the notebook to function.\n",
    "\n",
    "from world import Environment\n",
    "from world import Grid\n",
    "from world import agent_vision\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "plt.ion()\n",
    "\n",
    "# setting the device\n",
    "# if gpu is to be used for Mac OS\n",
    "# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "# if gpu is to be used for cuda\n",
    "\n",
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure of the Experiences to store\n",
    "Experience = namedtuple('Experience', \n",
    "('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# stores the Experience Replay buffer\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory= deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # Save the Experience into memory \n",
    "        self.memory.append(Experience(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        #selecting a random batch of Experience for training\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change the model to a CNN so that it can work with the agent vision input.\n",
    "\n",
    "\"\"\"Simple MLP network.\"\"\"\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            n_observations: observation/state size of the environment\n",
    "            n_actions: number of discrete actions available in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "        \"\"\"\n",
    "    def __init__(self, n_vision, n_actions, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_vision, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, n_actions)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128      # BATCH_SIZE is the number of Experience sampled from the replay buffer\n",
    "GAMMA = 0.99          # GAMMA is the discount factor as mentioned in the previous section\n",
    "EPSILON_START = 0.9   # EPSILON_START is the starting value of epsilon\n",
    "EPSILON_END = 0.05    # EPSILON_END is the final value of epsilon\n",
    "EPSILON_DECAY = 1000  # EPSILON_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "TAU = 0.005           # TAU is the update rate of the target network\n",
    "LR = 1e-4             # LR is the learning rate of the AdamW optimizer\n",
    "HIDDEN_SIZE = 128     # the hidden layers in the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 2 4 0 1]\n",
      " [1 0 0 0 2 0 2 0 0 1]\n",
      " [1 0 0 2 3 0 2 0 0 1]\n",
      " [1 0 0 0 2 2 2 0 0 1]\n",
      " [1 0 0 0 0 3 2 3 0 1]\n",
      " [1 0 2 2 2 0 2 2 0 1]\n",
      " [1 0 0 3 2 0 0 0 0 1]\n",
      " [1 0 0 0 2 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]] {'dirt_cleaned': [0], 'agent_moved': [False], 'agent_charging': [False], 'agent_pos': [(5, 3)]}\n"
     ]
    }
   ],
   "source": [
    "env = Environment(\n",
    "    grid_fp=Path('grid_configs/multi_room.grd'),\n",
    "    n_agents=1,\n",
    "    agent_start_pos=None,\n",
    "    sigma=0.0,\n",
    "    no_gui=True,\n",
    "    # target_fps=fps,\n",
    "    random_seed=0,\n",
    "    reward_fn='custom',\n",
    ")\n",
    "\n",
    "obs, info = env.get_observation()\n",
    "\n",
    "print(obs, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    epsilon_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * math.exp(-1. * steps_done / EPSILON_DECAY)\n",
    "    \n",
    "    steps_done += 1\n",
    "    if sample > epsilon_threshold:\n",
    "        # max(1) will return largest column value of each row.\n",
    "        # second column on max result is index of where max element was found\n",
    "        # so we pick action with the larger expected reward.\n",
    "        with torch.no_grad():\n",
    "            return torch.tensor([torch.argmax(policy_network(state))])\n",
    "    else:\n",
    "        return torch.tensor([random.randint(0,3)], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(show_result=False):\n",
    "    '''\n",
    "    Function to plot the durations of episodes, \n",
    "    along with an average over the last 100 episodes\n",
    "    '''\n",
    "    plt.figure(1)\n",
    "    duration_t = torch.tensor(episode_duration, dtype= torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(duration_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(duration_t) >= 100:\n",
    "        means = duration_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "    else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that performs a single step of the optimization\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transition = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    '''\n",
    "    # Transpose the batch and convert the  batch-array of Experience\n",
    "    # to Transition of batch-arrays\n",
    "    '''\n",
    "    batch = Experience(*zip(*transition))\n",
    "    #print('batch:', batch)\n",
    "    #Compute a mask of non-final states and concatenate the batch element\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.stack([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.stack(batch.state)\n",
    "    # print('state batch:', state_batch, len(state_batch))\n",
    "    action_batch = torch.stack(batch.action)\n",
    "    # print('action batch:', action_batch)\n",
    "    reward_batch = torch.stack(batch.reward)\n",
    "    # print('reward batch:', reward_batch)\n",
    "\n",
    "    \n",
    "    # Compute Q(s_t, a)\n",
    "    #These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_network(state_batch).gather(1,action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    next_state_values = torch.zeros(BATCH_SIZE,device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_network(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = reward_batch + (GAMMA* next_state_values)\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    criterion=nn.SmoothL1Loss()\n",
    "    loss=criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    #optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_network.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 2 4 0 1]\n",
      " [1 0 0 0 2 0 2 0 0 1]\n",
      " [1 0 0 2 3 0 2 0 0 1]\n",
      " [1 0 0 0 2 2 2 0 0 1]\n",
      " [1 0 0 0 0 3 2 3 0 1]\n",
      " [1 0 2 2 2 0 2 2 0 1]\n",
      " [1 0 0 3 2 0 0 0 0 1]\n",
      " [1 0 0 0 2 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]]\n",
      "{'dirt_cleaned': [0], 'agent_moved': [False], 'agent_charging': [False], 'agent_pos': [(3, 8)]}\n",
      "[[-1 -1  0 -1 -1]\n",
      " [-1  0  0  1 -1]\n",
      " [ 2  0  0  1 -1]\n",
      " [-1  0  0  1 -1]\n",
      " [-1 -1  0 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Get the agent_vision function working.\n",
    "\n",
    "env.reset()\n",
    "obs, info = env.get_observation()\n",
    "print(obs)\n",
    "print(info)\n",
    "\n",
    "agent_pos = info['agent_pos']\n",
    "\n",
    "g = Grid(obs.shape[0], obs.shape[1])\n",
    "g.cells = obs\n",
    "#print(agent_pos)\n",
    "agent_view = agent_vision(loc=agent_pos[0], grid=g, vis_range=2)\n",
    "print(agent_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "Room is not clean yet so charging is not allowed.\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 71\u001b[0m\n\u001b[0;32m     67\u001b[0m obs \u001b[39m=\u001b[39m next_obs\n\u001b[0;32m     69\u001b[0m \u001b[39m# Perform one step of the optimization (on the policy network)\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39m# The agent performs an optimization step on the Policy Network using the stored memory\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m optimize_model()\n\u001b[0;32m     73\u001b[0m \u001b[39m\u001b[39m\u001b[39m'''\u001b[39;00m\n\u001b[0;32m     74\u001b[0m \u001b[39mThe agent will perform a soft update of the Target Network's weights, \u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[39mwith the equation TAU * policy_net_state_dict + (1-TAU) * target_net_state_dict, \u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[39mthis helps to make the Target Network's weights converge to the Policy Network's weights.\u001b[39;00m\n\u001b[0;32m     77\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[39m# Soft update of the target network's weights\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[39m# θ′ ← τ θ + (1 −τ )θ′\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[39m# policy_network.state_dict() returns the parameters of the policy network\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[39m# target_network.load_state_dict() loads these parameters into the target network.\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[58], line 31\u001b[0m, in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m reward_batch \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstack(batch\u001b[39m.\u001b[39mreward)\n\u001b[0;32m     25\u001b[0m \u001b[39m# print('reward batch:', reward_batch)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[39m# Compute Q(s_t, a)\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[39m#These are the actions which would've been taken\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[39m# for each batch state according to policy_net\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m state_action_values \u001b[39m=\u001b[39m policy_network(state_batch)\u001b[39m.\u001b[39mgather(\u001b[39m1\u001b[39m,action_batch)\n\u001b[0;32m     33\u001b[0m \u001b[39m# Compute V(s_{t+1}) for all next states.\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[39m# Expected values of actions for non_final_next_states are computed based\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[39m# on the \"older\" target_net; selecting their best reward with max(1)[0].\u001b[39;00m\n\u001b[0;32m     36\u001b[0m next_state_values \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros(BATCH_SIZE,device\u001b[39m=\u001b[39mdevice)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[30], line 18\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[0;32m     17\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1(x))\n\u001b[1;32m---> 18\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(x))\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#TODO: Change agent_vision code if needed.\n",
    "\n",
    "n_actions = 4\n",
    "vis_range = 2\n",
    "n_vision = 25\n",
    "max_steps = 1000\n",
    "\n",
    "env.reset()\n",
    "obs, info = env.get_observation()\n",
    "agent_pos = info['agent_pos']\n",
    "\n",
    "g = Grid(obs.shape[0], obs.shape[1])\n",
    "g.cells = obs\n",
    "agent_view = agent_vision(loc=agent_pos[0], grid=g, vis_range=vis_range)\n",
    "agent_view = torch.flatten(torch.from_numpy(agent_view)).to(device)\n",
    "\n",
    "policy_network = DQN(n_vision, n_actions, HIDDEN_SIZE).to(device)\n",
    "target_network = DQN(n_vision, n_actions, HIDDEN_SIZE).to(device)\n",
    "#updates the parameters of the target network  with the parameters of the policy network \n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_network.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0\n",
    "episode_duration = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 5\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    print(i)\n",
    "    #At the beginning we reset the environment and \n",
    "    #initialize the state Tensor.\n",
    "    env.reset()\n",
    "    obs, info = env.get_observation()\n",
    "    agent_pos = info['agent_pos']\n",
    "    g = Grid(obs.shape[0], obs.shape[1])\n",
    "    g.cells = obs\n",
    "    agent_view = agent_vision(loc=agent_pos[0], grid=g, vis_range=vis_range)\n",
    "    agent_view = torch.flatten(torch.from_numpy(agent_view)).to(device)\n",
    "    state = agent_view.float()\n",
    "    # print('state:', state)\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Sample an action\n",
    "        action = choose_action(state).to(device)\n",
    "        # Execute it, observe the next state and the reward\n",
    "        next_obs, reward, terminated, info = env.step([action])\n",
    "        reward = torch.tensor([reward], device=device, dtype=torch.long)\n",
    "        next_agent_pos = info['agent_pos']\n",
    "        next_g = Grid(next_obs.shape[0], next_obs.shape[1])\n",
    "        next_g.cells = next_obs\n",
    "        next_agent_view = agent_vision(loc=next_agent_pos[0], grid=next_g, vis_range=vis_range)\n",
    "        if terminated:\n",
    "            break\n",
    "        else:\n",
    "            next_state = torch.flatten(torch.from_numpy(next_agent_view)).float().to(device)\n",
    "\n",
    "        # Store the experience in the memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state and grid\n",
    "        state = next_state\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        # The agent performs an optimization step on the Policy Network using the stored memory\n",
    "        optimize_model()\n",
    "\n",
    "        '''\n",
    "        The agent will perform a soft update of the Target Network's weights, \n",
    "        with the equation TAU * policy_net_state_dict + (1-TAU) * target_net_state_dict, \n",
    "        this helps to make the Target Network's weights converge to the Policy Network's weights.\n",
    "        '''\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        # policy_network.state_dict() returns the parameters of the policy network\n",
    "        # target_network.load_state_dict() loads these parameters into the target network.\n",
    "        target_net_state_dict = target_network.state_dict()\n",
    "        policy_net_state_dict = policy_network.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_network.load_state_dict(target_net_state_dict)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated:\n",
    "            episode_duration.append(t + 1)\n",
    "            print(\"Episode\", i,\"Game terminated after\", t, \"steps with reward\", total_reward)\n",
    "            \n",
    "            break\n",
    "        \n",
    "print('Complete')\n",
    "plot_scores(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
