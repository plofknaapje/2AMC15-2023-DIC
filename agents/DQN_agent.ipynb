{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is based on and partly copied from these two sources: \n",
    "- https://arshren.medium.com/deep-q-learning-a-deep-reinforcement-learning-algorithm-f1366cf1b53d\n",
    "- https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "NOTE: Do not forget to change this notebook into a .py file before submission. Also, save and load the model after it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "# import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\dough\\\\Documents\\\\DIC\\\\2AMC15-2'"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "new_path = cwd[:-7]\n",
    "new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('c:\\\\Users\\\\dough\\\\Documents\\\\DIC\\\\2AMC15-2023-DIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Fix the imports below, since they are required for the notebook to function.\n",
    "\n",
    "from world import Environment\n",
    "from world import Grid\n",
    "from world import agent_vision\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "plt.ion()\n",
    "\n",
    "# setting the device\n",
    "# if gpu is to be used for Mac OS\n",
    "# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "# if gpu is to be used for cuda\n",
    "\n",
    "#device = torch.device('cpu')\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure of the Experiences to store\n",
    "Experience = namedtuple('Experience', \n",
    "('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# stores the Experience Replay buffer\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory= deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # Save the Experience into memory \n",
    "        self.memory.append(Experience(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        #selecting a random batch of Experience for training\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change the model to a CNN so that it can work with the agent vision input.\n",
    "\n",
    "\"\"\"Simple MLP network.\"\"\"\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            n_observations: observation/state size of the environment\n",
    "            n_actions: number of discrete actions available in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "        \"\"\"\n",
    "    def __init__(self, n_vision, n_actions, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_vision, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear(hidden_size, n_actions)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128      # BATCH_SIZE is the number of Experience sampled from the replay buffer\n",
    "GAMMA = 0.99          # GAMMA is the discount factor as mentioned in the previous section\n",
    "EPSILON_START = 0.9   # EPSILON_START is the starting value of epsilon\n",
    "EPSILON_END = 0.05    # EPSILON_END is the final value of epsilon\n",
    "EPSILON_DECAY = 1000  # EPSILON_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "TAU = 0.005           # TAU is the update rate of the target network\n",
    "LR = 1e-4             # LR is the learning rate of the AdamW optimizer\n",
    "HIDDEN_SIZE = 128     # the hidden layers in the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 2 4 0 1]\n",
      " [1 0 0 0 2 0 2 0 0 1]\n",
      " [1 0 0 2 3 0 2 0 0 1]\n",
      " [1 0 0 0 2 2 2 0 0 1]\n",
      " [1 0 0 0 0 3 2 3 0 1]\n",
      " [1 0 2 2 2 0 2 2 0 1]\n",
      " [1 0 0 3 2 0 0 0 0 1]\n",
      " [1 0 0 0 2 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]] {'dirt_cleaned': [0], 'agent_moved': [False], 'agent_charging': [False], 'agent_pos': [(5, 3)], 'agent_pos_arr': [array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int8)], 'dirt_vecs': [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]}\n"
     ]
    }
   ],
   "source": [
    "env = Environment(\n",
    "    grid_fp=Path('grid_configs/multi_room.grd'),\n",
    "    n_agents=1,\n",
    "    agent_start_pos=None,\n",
    "    sigma=0.0,\n",
    "    no_gui=True,\n",
    "    # target_fps=fps,\n",
    "    random_seed=0,\n",
    "    reward_fn='custom',\n",
    ")\n",
    "\n",
    "obs, info = env.get_observation()\n",
    "\n",
    "print(obs, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state, policy_network):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    epsilon_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * math.exp(-1. * steps_done / EPSILON_DECAY)\n",
    "    print(state)\n",
    "    print(state.shape)\n",
    "    steps_done += 1\n",
    "    if sample > epsilon_threshold:\n",
    "        # max(1) will return largest column value of each row.\n",
    "        # second column on max result is index of where max element was found\n",
    "        # so we pick action with the larger expected reward.\n",
    "        with torch.no_grad():\n",
    "            return torch.tensor([torch.argmax(policy_network(state))])\n",
    "    else:\n",
    "        return torch.tensor([random.randint(0,3)], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(show_result=False):\n",
    "    '''\n",
    "    Function to plot the durations of episodes, \n",
    "    along with an average over the last 100 episodes\n",
    "    '''\n",
    "    plt.figure(1)\n",
    "    duration_t = torch.tensor(episode_duration, dtype= torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(duration_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(duration_t) >= 100:\n",
    "        means = duration_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "    else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that performs a single step of the optimization\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transition = memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    '''\n",
    "    # Transpose the batch and convert the  batch-array of Experience\n",
    "    # to Transition of batch-arrays\n",
    "    '''\n",
    "    batch = Experience(*zip(*transition))\n",
    "    #print('batch:', batch)\n",
    "    #Compute a mask of non-final states and concatenate the batch element\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.stack([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch = torch.stack(batch.state)\n",
    "    # print('state batch:', state_batch, len(state_batch))\n",
    "    action_batch = torch.stack(batch.action)\n",
    "    # print('action batch:', action_batch)\n",
    "    reward_batch = torch.stack(batch.reward)\n",
    "    # print('reward batch:', reward_batch)\n",
    "\n",
    "    \n",
    "    # Compute Q(s_t, a)\n",
    "    #These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_network(state_batch).gather(1,action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    next_state_values = torch.zeros(BATCH_SIZE,device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_network(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = reward_batch + (GAMMA* next_state_values)\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    criterion=nn.SmoothL1Loss()\n",
    "    loss=criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    #optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_network.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1  0 -1 -1]\n",
      " [-1  0  0  1 -1]\n",
      " [ 2  0  0  2 -1]\n",
      " [-1  2  0  1 -1]\n",
      " [-1 -1  0 -1 -1]]\n",
      "[[[0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 1 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]\n",
      "  [0 0 0 0 0 0 0 0 0 0]]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0.]]\n",
      "tensor([[-1., -1.,  0., -1., -1., -1.,  0.,  0.,  1., -1.,  2.,  0.,  0.,  2.,\n",
      "         -1., -1.,  2.,  0.,  1., -1., -1., -1.,  0., -1., -1.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "#TODO: Get the agent_vision function working.\n",
    "\n",
    "env.reset()\n",
    "obs, info = env.get_observation()\n",
    "#print(obs)\n",
    "#print(info)\n",
    "\n",
    "agent_pos = info['agent_pos']\n",
    "agent_pos_arr = np.array(info['agent_pos_arr'])\n",
    "dirt_vecs = np.array(info['dirt_vecs'])\n",
    "\n",
    "g = Grid(obs.shape[0], obs.shape[1])\n",
    "g.cells = obs\n",
    "#print(agent_pos)\n",
    "agent_view = agent_vision(loc=agent_pos[0], grid=g, vis_range=2)\n",
    "\n",
    "print(agent_view)\n",
    "print(agent_pos_arr)\n",
    "print(dirt_vecs)\n",
    "agent_view = torch.flatten(torch.from_numpy(agent_view))\n",
    "agent_pos_arr = torch.flatten(torch.from_numpy(agent_pos_arr))\n",
    "dirt_vecs = torch.flatten(torch.from_numpy(dirt_vecs))\n",
    "agent_input = torch.cat((agent_view, agent_pos_arr, dirt_vecs)).unsqueeze(0)\n",
    "print(agent_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Broken fix \n",
    "\n",
    "def agent_input(obs, info, vis_range):\n",
    "    agent_pos = info['agent_pos']\n",
    "    g = Grid(obs.shape[0], obs.shape[1])\n",
    "    g.cells = obs\n",
    "    agent_view = agent_vision(loc=agent_pos[0], grid=g, vis_range=vis_range)\n",
    "    \n",
    "    \n",
    "    agent_view = torch.flatten(torch.from_numpy(agent_view))\n",
    "    \n",
    "    agent_pos_arr = torch.tensor(np.vstack(info['agent_pos_arr']).astype(float))\n",
    "    agent_pos_arr = torch.flatten(agent_pos_arr)\n",
    "    dirt_vecs = torch.FloatTensor(info['dirt_vecs'])\n",
    "    dirt_vecs = torch.flatten(dirt_vecs)\n",
    "    \n",
    "    return torch.cat((agent_view, agent_pos_arr, dirt_vecs)).unsqueeze(0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "state: torch.Size([1, 155])\n",
      "1 loop\n",
      "stateyyyy: torch.Size([1, 155])\n",
      "tensor([[-1., -1.,  0., -1., -1., -1.,  0.,  0.,  2., -1.,  2.,  0.,  0.,  0.,\n",
      "          2., -1.,  0.,  0.,  0., -1., -1., -1.,  2., -1., -1.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.]], device='cuda:0')\n",
      "torch.Size([1, 155])\n",
      "next_obs (10, 10)\n",
      "next_info {'dirt_cleaned': [0], 'agent_moved': [True], 'agent_charging': [False], 'agent_pos': [(4, 1)], 'agent_pos_arr': [None], 'dirt_vecs': [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]}\n",
      "shapeyyyy torch.Size([1, 155])\n",
      "next_state torch.Size([1, 56])\n",
      "1 loop\n",
      "stateyyyy: torch.Size([1, 56])\n",
      "tensor([[-1., -1.,  0., -1., -1., -1.,  1.,  0.,  0., -1., -1.,  2.,  0.,  0.,\n",
      "          0., -1.,  1.,  0.,  0., -1., -1., -1.,  0., -1., -1., nan,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 56])\n",
      "next_obs (10, 10)\n",
      "next_info {'dirt_cleaned': [0], 'agent_moved': [True], 'agent_charging': [False], 'agent_pos': [(3, 1)], 'agent_pos_arr': [None], 'dirt_vecs': [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]}\n",
      "shapeyyyy torch.Size([1, 56])\n",
      "next_state torch.Size([1, 56])\n",
      "1 loop\n",
      "stateyyyy: torch.Size([1, 56])\n",
      "tensor([[-1., -1.,  0., -1., -1., -1.,  1.,  0.,  0., -1., -1.,  2.,  0.,  0.,\n",
      "          2., -1.,  1.,  0.,  0., -1., -1., -1.,  0., -1., -1., nan,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 56])\n",
      "next_obs (10, 10)\n",
      "next_info {'dirt_cleaned': [0], 'agent_moved': [True], 'agent_charging': [False], 'agent_pos': [(3, 2)], 'agent_pos_arr': [None], 'dirt_vecs': [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]}\n",
      "shapeyyyy torch.Size([1, 56])\n",
      "next_state torch.Size([1, 56])\n",
      "1 loop\n",
      "stateyyyy: torch.Size([1, 56])\n",
      "tensor([[-1., -1.,  0., -1., -1., -1.,  0.,  0.,  0., -1.,  2.,  0.,  0.,  2.,\n",
      "         -1., -1.,  0.,  0.,  0., -1., -1., -1.,  0., -1., -1., nan,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 56])\n",
      "next_obs (10, 10)\n",
      "next_info {'dirt_cleaned': [0], 'agent_moved': [True], 'agent_charging': [False], 'agent_pos': [(2, 2)], 'agent_pos_arr': [None], 'dirt_vecs': [array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])]}\n",
      "shapeyyyy torch.Size([1, 56])\n",
      "next_state torch.Size([1, 56])\n",
      "1 loop\n",
      "stateyyyy: torch.Size([1, 56])\n",
      "tensor([[-1., -1.,  2., -1., -1., -1.,  0.,  0.,  0., -1.,  2.,  0.,  0.,  0.,\n",
      "          2., -1.,  0.,  0.,  2., -1., -1., -1.,  0., -1., -1., nan,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,\n",
      "          0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n",
      "       device='cuda:0')\n",
      "torch.Size([1, 56])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (1x56 and 155x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 51\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m1 loop\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mstateyyyy:\u001b[39m\u001b[39m'\u001b[39m, state\u001b[39m.\u001b[39mshape)\n\u001b[1;32m---> 51\u001b[0m action \u001b[39m=\u001b[39m choose_action(state , policy_network)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     52\u001b[0m \u001b[39m# Execute it, observe the next state and the reward\u001b[39;00m\n\u001b[0;32m     53\u001b[0m next_obs, reward, terminated, next_info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep([action])\n",
      "Cell \u001b[1;32mIn[158], line 13\u001b[0m, in \u001b[0;36mchoose_action\u001b[1;34m(state, policy_network)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mif\u001b[39;00m sample \u001b[39m>\u001b[39m epsilon_threshold:\n\u001b[0;32m      9\u001b[0m     \u001b[39m# max(1) will return largest column value of each row.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m     \u001b[39m# second column on max result is index of where max element was found\u001b[39;00m\n\u001b[0;32m     11\u001b[0m     \u001b[39m# so we pick action with the larger expected reward.\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m---> 13\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor([torch\u001b[39m.\u001b[39margmax(policy_network(state))])\n\u001b[0;32m     14\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     15\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor([random\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m,\u001b[39m3\u001b[39m)], device\u001b[39m=\u001b[39mdevice, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[155], line 17\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m,x):\n\u001b[1;32m---> 17\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(x))\n\u001b[0;32m     18\u001b[0m     x \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2(x))\n\u001b[0;32m     19\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3(x)\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Program Files\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (1x56 and 155x128)"
     ]
    }
   ],
   "source": [
    "#TODO: Change agent_vision code if needed.\n",
    "\n",
    "n_actions = 4\n",
    "vis_range = 2\n",
    "n_vision = 155\n",
    "max_steps = 1000\n",
    "\n",
    "env.reset()\n",
    "obs, info = env.get_observation()\n",
    "agent_pos = info['agent_pos']\n",
    "\n",
    "#g = Grid(obs.shape[0], obs.shape[1])\n",
    "g.cells = obs\n",
    "\n",
    "\n",
    "state = agent_input(obs, info, vis_range).to(device)\n",
    "\n",
    "policy_network = DQN(n_vision, n_actions, HIDDEN_SIZE).to(device)\n",
    "target_network = DQN(n_vision, n_actions, HIDDEN_SIZE).to(device)\n",
    "#updates the parameters of the target network  with the parameters of the policy network \n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_network.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0\n",
    "episode_duration = []\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 5\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    print(i)\n",
    "    #At the beginning we reset the environment and \n",
    "    #initialize the state Tensor.\n",
    "    env.reset()\n",
    "    obs, info = env.get_observation()\n",
    "    agent_pos = info['agent_pos']\n",
    "    #g = Grid(obs.shape[0], obs.shape[1])\n",
    "    #g.cells = obs\n",
    "\n",
    "    state = agent_input(obs, info, vis_range).to(device).float()\n",
    "    print('state:', state.shape)\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in range(max_steps):\n",
    "        # Sample an action\n",
    "        print('1 loop')\n",
    "        print('stateyyyy:', state.shape)\n",
    "        action = choose_action(state , policy_network).to(device)\n",
    "        # Execute it, observe the next state and the reward\n",
    "        next_obs, reward, terminated, next_info = env.step([action])\n",
    "        reward = torch.tensor([reward], device=device, dtype=torch.long)\n",
    "        \n",
    "        print('next_obs', next_obs.shape)\n",
    "        print('next_info', next_info)\n",
    "        if terminated:\n",
    "            break\n",
    "        else:\n",
    "            next_state = agent_input(next_obs, next_info, vis_range).to(device).float()\n",
    "\n",
    "        # Store the experience in the memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        print('shapeyyyy',state.shape)\n",
    "        # Move to the next state and grid\n",
    "        print('next_state', next_state.shape)\n",
    "        state = next_state\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        # The agent performs an optimization step on the Policy Network using the stored memory\n",
    "        optimize_model()\n",
    "\n",
    "        '''\n",
    "        The agent will perform a soft update of the Target Network's weights, \n",
    "        with the equation TAU * policy_net_state_dict + (1-TAU) * target_net_state_dict, \n",
    "        this helps to make the Target Network's weights converge to the Policy Network's weights.\n",
    "        '''\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        # policy_network.state_dict() returns the parameters of the policy network\n",
    "        # target_network.load_state_dict() loads these parameters into the target network.\n",
    "        target_net_state_dict = target_network.state_dict()\n",
    "        policy_net_state_dict = policy_network.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_network.load_state_dict(target_net_state_dict)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated:\n",
    "            episode_duration.append(t + 1)\n",
    "            print(\"Episode\", i,\"Game terminated after\", t, \"steps with reward\", total_reward)\n",
    "            \n",
    "            break\n",
    "        \n",
    "print('Complete')\n",
    "plot_scores(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
