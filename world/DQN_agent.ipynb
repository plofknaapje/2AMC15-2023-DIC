{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is based on and partly copied from these two sources: \n",
    "- https://arshren.medium.com/deep-q-learning-a-deep-reinforcement-learning-algorithm-f1366cf1b53d\n",
    "- https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "NOTE: Do not forget to change this notebook into a .py file before submission. Also, save and load the model after it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "# import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\20174216\\\\OneDrive - TU Eindhoven\\\\Documents\\\\1. TUe MSc DSAI\\\\Q4\\\\2AMC15 - Data Intelligence Challenge\\\\2AMC15-2023-DIC\\\\world'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "# new_path = cwd[:-8]\n",
    "# new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('c:\\\\Users\\\\20174216\\\\OneDrive - TU Eindhoven\\\\Documents\\\\1. TUe MSc DSAI\\\\Q4\\\\2AMC15 - Data Intelligence Challenge\\\\2AMC15-2023-DIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.10.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#TODO: Fix the imports below, since they are required for the notebook to function.\n",
    "\n",
    "from world import Environment\n",
    "from world import Grid\n",
    "from world import agent_vision\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "plt.ion()\n",
    "\n",
    "# setting the device\n",
    "# if gpu is to be used for Mac OS\n",
    "# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "# if gpu is to be used for cuda\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure of the Experiences to store\n",
    "Experience = namedtuple('Experience', \n",
    "('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# stores the Experience Replay buffer\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory= deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # Save the Experience into memory \n",
    "        self.memory.append(Experience(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        #selecting a random batch of Experience for training\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Simple MLP network.\"\"\"\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            n_observations: observation/state size of the environment\n",
    "            n_actions: number of discrete actions available in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "        \"\"\"\n",
    "    def __init__(self, n_observations, n_actions, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear( hidden_size, n_actions)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128      # BATCH_SIZE is the number of Experience sampled from the replay buffer\n",
    "GAMMA = 0.99          # GAMMA is the discount factor as mentioned in the previous section\n",
    "EPSILON_START = 0.9   # EPSILON_START is the starting value of epsilon\n",
    "EPSILON_END = 0.05    # EPSILON_END is the final value of epsilon\n",
    "EPSILON_DECAY = 1000  # EPSILON_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "TAU = 0.005           # TAU is the update rate of the target network\n",
    "LR = 1e-4             # LR is the learning rate of the AdamW optimizer\n",
    "HIDDEN_SIZE = 128     # the hidden layers in the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 2 4 0 1]\n",
      " [1 0 0 0 2 0 2 0 0 1]\n",
      " [1 0 0 2 3 0 2 0 0 1]\n",
      " [1 0 0 0 2 2 2 0 0 1]\n",
      " [1 0 0 0 0 3 2 3 0 1]\n",
      " [1 0 2 2 2 0 2 2 0 1]\n",
      " [1 0 0 3 2 0 0 0 0 1]\n",
      " [1 0 0 0 2 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]] {'dirt_cleaned': [0], 'agent_moved': [False], 'agent_charging': [False], 'agent_pos': [(5, 3)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20174216\\OneDrive - TU Eindhoven\\Documents\\1. TUe MSc DSAI\\Q4\\2AMC15 - Data Intelligence Challenge\\2AMC15-2023-DIC\\world\\environment.py:183: UserWarning: No initial agent positions given. Randomly placing agents on the grid.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "env = Environment(\n",
    "    grid_fp=Path('grid_configs/multi_room.grd'),\n",
    "    n_agents=1,\n",
    "    agent_start_pos=None,\n",
    "    sigma=0.0,\n",
    "    no_gui=True,\n",
    "    # target_fps=fps,\n",
    "    random_seed=0,\n",
    "    reward_fn='custom',\n",
    ")\n",
    "\n",
    "obs, info = env.get_observation()\n",
    "\n",
    "print(obs, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    epsilon_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * math.exp(-1. * steps_done / EPSILON_DECAY)\n",
    "    \n",
    "    steps_done += 1\n",
    "    if sample > epsilon_threshold:\n",
    "        # max(1) will return largest column value of each row.\n",
    "        # second column on max result is index of where max element was found\n",
    "        # so we pick action with the larger expected reward.\n",
    "        with torch.no_grad():\n",
    "            return policy_network(state).max(1)[1].view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randint(0,3)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(show_result=False):\n",
    "    '''\n",
    "    Function to plot the durations of episodes, \n",
    "    along with an average over the last 100 episodes\n",
    "    '''\n",
    "    plt.figure(1)\n",
    "    duration_t = torch.tensor(episode_duration, dtype= torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(duration_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(duration_t) >= 100:\n",
    "        means = duration_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "    else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that performs a single step of the optimization\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transition= memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    '''\n",
    "    # Transpose the batch and convert the  batch-array of Experience\n",
    "    # to Transition of batch-arrays\n",
    "    '''\n",
    "    batch=Experience(*zip(*transition))\n",
    "    \n",
    "    #Compute a mask of non-final states and concatenate the batch element\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch=torch.cat(batch.state)\n",
    "    action_batch=torch.cat(batch.action)\n",
    "    reward_batch=torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a)\n",
    "    #These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_network(state_batch).gather(1,action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    next_state_values = torch.zeros(BATCH_SIZE,device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_network(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = reward_batch + (GAMMA* next_state_values)\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    criterion=nn.SmoothL1Loss()\n",
    "    loss=criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    #optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_network.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 2 4 0 1]\n",
      " [1 0 0 0 2 0 2 0 0 1]\n",
      " [1 0 0 2 3 0 2 0 0 1]\n",
      " [1 0 0 0 2 2 2 0 0 1]\n",
      " [1 0 0 0 0 3 2 3 0 1]\n",
      " [1 0 2 2 2 0 2 2 0 1]\n",
      " [1 0 0 3 2 0 0 0 0 1]\n",
      " [1 0 0 0 2 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]]\n",
      "{'dirt_cleaned': [0], 'agent_moved': [False], 'agent_charging': [False], 'agent_pos': [(5, 8)]}\n",
      "[[-1 -1 -1]\n",
      " [-1 -1 -1]\n",
      " [-1 -1 -1]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m g \u001b[39m=\u001b[39m Grid(obs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], obs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m     11\u001b[0m g\u001b[39m.\u001b[39mcells \u001b[39m=\u001b[39m obs\n\u001b[1;32m---> 13\u001b[0m agent_view \u001b[39m=\u001b[39m agent_vision(loc\u001b[39m=\u001b[39;49magent_pos, grid\u001b[39m=\u001b[39;49mg, vis_range\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\20174216\\OneDrive - TU Eindhoven\\Documents\\1. TUe MSc DSAI\\Q4\\2AMC15 - Data Intelligence Challenge\\2AMC15-2023-DIC\\world\\vision.py:25\u001b[0m, in \u001b[0;36magent_vision\u001b[1;34m(loc, grid, vis_range)\u001b[0m\n\u001b[0;32m     21\u001b[0m vision \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull(\n\u001b[0;32m     22\u001b[0m     shape\u001b[39m=\u001b[39m(vis_range \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, vis_range \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m),\n\u001b[0;32m     23\u001b[0m     fill_value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39mprint\u001b[39m(vision)\n\u001b[1;32m---> 25\u001b[0m vision[center[\u001b[39m0\u001b[39;49m], center[\u001b[39m1\u001b[39;49m]] \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mcells[loc]\n\u001b[0;32m     27\u001b[0m \u001b[39m# Straight lines from center\u001b[39;00m\n\u001b[0;32m     28\u001b[0m directions \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]), np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]),\n\u001b[0;32m     29\u001b[0m               np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]), np\u001b[39m.\u001b[39marray([\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m])]\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#TODO: Get the agent_vision function working.\n",
    "\n",
    "env.reset()\n",
    "obs, info = env.get_observation()\n",
    "print(obs)\n",
    "print(info)\n",
    "\n",
    "agent_pos = info['agent_pos']\n",
    "\n",
    "g = Grid(obs.shape[0], obs.shape[1])\n",
    "g.cells = obs\n",
    "\n",
    "agent_view = agent_vision(loc=agent_pos, grid=g, vis_range=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.cells[agent_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m g \u001b[39m=\u001b[39m Grid(obs\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], obs\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m])\n\u001b[0;32m      9\u001b[0m g\u001b[39m.\u001b[39mcells \u001b[39m=\u001b[39m obs\n\u001b[1;32m---> 10\u001b[0m agent_view \u001b[39m=\u001b[39m agent_vision(loc\u001b[39m=\u001b[39;49magent_pos, grid\u001b[39m=\u001b[39;49mg, vis_range\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m)\n\u001b[0;32m     11\u001b[0m input_shape \u001b[39m=\u001b[39m agent_view\u001b[39m.\u001b[39mshape\n\u001b[0;32m     13\u001b[0m policy_network \u001b[39m=\u001b[39m DQN(input_shape, n_actions, HIDDEN_SIZE)\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\20174216\\OneDrive - TU Eindhoven\\Documents\\1. TUe MSc DSAI\\Q4\\2AMC15 - Data Intelligence Challenge\\2AMC15-2023-DIC\\world\\vision.py:25\u001b[0m, in \u001b[0;36magent_vision\u001b[1;34m(loc, grid, vis_range)\u001b[0m\n\u001b[0;32m     21\u001b[0m vision \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mfull(\n\u001b[0;32m     22\u001b[0m     shape\u001b[39m=\u001b[39m(vis_range \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, vis_range \u001b[39m*\u001b[39m \u001b[39m2\u001b[39m \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m),\n\u001b[0;32m     23\u001b[0m     fill_value\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     24\u001b[0m \u001b[39mprint\u001b[39m(vision)\n\u001b[1;32m---> 25\u001b[0m vision[center[\u001b[39m0\u001b[39;49m], center[\u001b[39m1\u001b[39;49m]] \u001b[39m=\u001b[39m grid\u001b[39m.\u001b[39mcells[loc]\n\u001b[0;32m     27\u001b[0m \u001b[39m# Straight lines from center\u001b[39;00m\n\u001b[0;32m     28\u001b[0m directions \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]), np\u001b[39m.\u001b[39marray([\u001b[39m0\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]),\n\u001b[0;32m     29\u001b[0m               np\u001b[39m.\u001b[39marray([\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m]), np\u001b[39m.\u001b[39marray([\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m])]\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "#TODO: Change agent_vision code if needed.\n",
    "\n",
    "n_actions = 4\n",
    "vis_range = 3\n",
    "env.reset()\n",
    "obs, info = env.get_observation()\n",
    "agent_pos = info['agent_pos']\n",
    "g = Grid(obs.shape[0], obs.shape[1])\n",
    "g.cells = obs\n",
    "agent_view = agent_vision(loc=agent_pos, grid=g, vis_range=3)\n",
    "input_shape = agent_view.shape\n",
    "\n",
    "policy_network = DQN(input_shape, n_actions, HIDDEN_SIZE).to(device)\n",
    "target_network = DQN(input_shape, n_actions, HIDDEN_SIZE).to(device)\n",
    "#updates the parameters of the target network  with the parameters of the policy network \n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_network.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0\n",
    "episode_duration = []\n",
    "num_episodes = 5\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    #At the beginning we reset the environment and \n",
    "    #initialize the state Tensor.\n",
    "    env.reset()\n",
    "    obs, info = env.get_observation()\n",
    "    agent_pos = info['agent_pos']\n",
    "    g = Grid(obs.shape[0], obs.shape[1])\n",
    "    g.cells = obs\n",
    "    agent_view = agent_vision(loc=agent_pos, grid=g, vis_range=vis_range)\n",
    "    state = torch.tensor(agent_view, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in count():\n",
    "        # Sample an action\n",
    "        action = choose_action(state)\n",
    "        # Execute it, observe the next state and the reward\n",
    "        next_obs, reward, terminated, info = env.step([action])\n",
    "        next_agent_pos = info['agent_pos']\n",
    "        next_g = Grid(next_obs.shape[0], next_obs.shape[1])\n",
    "        next_g.cells = next_obs\n",
    "        next_agent_view = agent_vision(loc=next_agent_pos, grid=next_g, vis_range=vis_range)\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(next_agent_view, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the experience in the memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state and grid\n",
    "        state = next_state\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        # The agent performs an optimization step on the Policy Network using the stored memory\n",
    "        optimize_model()\n",
    "\n",
    "        '''\n",
    "        The agent will perform a soft update of the Target Network's weights, \n",
    "        with the equation TAU * policy_net_state_dict + (1-TAU) * target_net_state_dict, \n",
    "        this helps to make the Target Network's weights converge to the Policy Network's weights.\n",
    "        '''\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        # policy_network.state_dict() returns the parameters of the policy network\n",
    "        # target_network.load_state_dict() loads these parameters into the target network.\n",
    "        target_net_state_dict = target_network.state_dict()\n",
    "        policy_net_state_dict = policy_network.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_network.load_state_dict(target_net_state_dict)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated:\n",
    "            episode_duration.append(t + 1)\n",
    "            print(\"Episode\", i,\"Game terminated after\", t, \"steps with reward\", total_reward)\n",
    "            \n",
    "            break\n",
    "        \n",
    "print('Complete')\n",
    "plot_scores(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
