{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q-Learning Agent"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code is based on and partly copied from these two sources: \n",
    "- https://arshren.medium.com/deep-q-learning-a-deep-reinforcement-learning-algorithm-f1366cf1b53d\n",
    "- https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "NOTE: Do not forget to change this notebook into a .py file before submission. Also, save and load the model after it has been trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "# import gym\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\20174216\\\\OneDrive - TU Eindhoven\\\\Documents\\\\1. TUe MSc DSAI\\\\Q4\\\\2AMC15 - Data Intelligence Challenge\\\\2AMC15-2023-DIC\\\\world'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "cwd\n",
    "# new_path = cwd[:-8]\n",
    "# new_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('c:\\\\Users\\\\20174216\\\\OneDrive - TU Eindhoven\\\\Documents\\\\1. TUe MSc DSAI\\\\Q4\\\\2AMC15 - Data Intelligence Challenge\\\\2AMC15-2023-DIC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.3.0 (SDL 2.24.2, Python 3.10.10)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "#TODO: Fix the imports below, since they are required for the notebook to function.\n",
    "\n",
    "from world import Environment\n",
    "from world import Grid\n",
    "from world import agent_vision\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "plt.ion()\n",
    "\n",
    "# setting the device\n",
    "# if gpu is to be used for Mac OS\n",
    "# device = torch.device('mps:0' if torch.backends.mps.is_available() else 'cpu')\n",
    "# if gpu is to be used for cuda\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# structure of the Experiences to store\n",
    "Experience = namedtuple('Experience', \n",
    "('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "# stores the Experience Replay buffer\n",
    "class ReplayMemory(object):\n",
    "    def __init__(self, capacity):\n",
    "        self.memory= deque([], maxlen=capacity)\n",
    "    \n",
    "    def push(self, *args):\n",
    "        # Save the Experience into memory \n",
    "        self.memory.append(Experience(*args))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        #selecting a random batch of Experience for training\n",
    "        return random.sample(self.memory,batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Change the model to a CNN so that it can work with the agent vision input.\n",
    "\n",
    "\"\"\"Simple MLP network.\"\"\"\n",
    "class DQN(nn.Module):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            n_observations: observation/state size of the environment\n",
    "            n_actions: number of discrete actions available in the environment\n",
    "            hidden_size: size of hidden layers\n",
    "        \"\"\"\n",
    "    def __init__(self, n_observations, n_actions, hidden_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, hidden_size)\n",
    "        self.layer2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.layer3 = nn.Linear( hidden_size, n_actions)\n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128      # BATCH_SIZE is the number of Experience sampled from the replay buffer\n",
    "GAMMA = 0.99          # GAMMA is the discount factor as mentioned in the previous section\n",
    "EPSILON_START = 0.9   # EPSILON_START is the starting value of epsilon\n",
    "EPSILON_END = 0.05    # EPSILON_END is the final value of epsilon\n",
    "EPSILON_DECAY = 1000  # EPSILON_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "TAU = 0.005           # TAU is the update rate of the target network\n",
    "LR = 1e-4             # LR is the learning rate of the AdamW optimizer\n",
    "HIDDEN_SIZE = 128     # the hidden layers in the DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 2 4 0 1]\n",
      " [1 0 0 0 2 0 2 0 0 1]\n",
      " [1 0 0 2 3 0 2 0 0 1]\n",
      " [1 0 0 0 2 2 2 0 0 1]\n",
      " [1 0 0 0 0 3 2 3 0 1]\n",
      " [1 0 2 2 2 0 2 2 0 1]\n",
      " [1 0 0 3 2 0 0 0 0 1]\n",
      " [1 0 0 0 2 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]] {'dirt_cleaned': [0], 'agent_moved': [False], 'agent_charging': [False], 'agent_pos': [(5, 3)]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\20174216\\OneDrive - TU Eindhoven\\Documents\\1. TUe MSc DSAI\\Q4\\2AMC15 - Data Intelligence Challenge\\2AMC15-2023-DIC\\world\\environment.py:183: UserWarning: No initial agent positions given. Randomly placing agents on the grid.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "env = Environment(\n",
    "    grid_fp=Path('grid_configs/multi_room.grd'),\n",
    "    n_agents=1,\n",
    "    agent_start_pos=None,\n",
    "    sigma=0.0,\n",
    "    no_gui=True,\n",
    "    # target_fps=fps,\n",
    "    random_seed=0,\n",
    "    reward_fn='custom',\n",
    ")\n",
    "\n",
    "obs, info = env.get_observation()\n",
    "\n",
    "print(obs, info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    epsilon_threshold = EPSILON_END + (EPSILON_START - EPSILON_END) * math.exp(-1. * steps_done / EPSILON_DECAY)\n",
    "    \n",
    "    steps_done += 1\n",
    "    if sample > epsilon_threshold:\n",
    "        # max(1) will return largest column value of each row.\n",
    "        # second column on max result is index of where max element was found\n",
    "        # so we pick action with the larger expected reward.\n",
    "        with torch.no_grad():\n",
    "            return policy_network(state).max(1)[1].view(1,1)\n",
    "    else:\n",
    "        return torch.tensor([[random.randint(0,3)]], device=device, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scores(show_result=False):\n",
    "    '''\n",
    "    Function to plot the durations of episodes, \n",
    "    along with an average over the last 100 episodes\n",
    "    '''\n",
    "    plt.figure(1)\n",
    "    duration_t = torch.tensor(episode_duration, dtype= torch.float)\n",
    "    if show_result:\n",
    "        plt.title('Result')\n",
    "    else:\n",
    "        plt.clf()\n",
    "        plt.title('Training...')\n",
    "    \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Duration')\n",
    "    plt.plot(duration_t.numpy())\n",
    "    # Take 100 episode averages and plot them too\n",
    "    if len(duration_t) >= 100:\n",
    "        means = duration_t.unfold(0, 100, 1).mean(1).view(-1)\n",
    "        means = torch.cat((torch.zeros(99), means))\n",
    "        plt.plot(means.numpy())\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    if not show_result:\n",
    "            display.display(plt.gcf())\n",
    "            display.clear_output(wait=True)\n",
    "    else:\n",
    "            display.display(plt.gcf())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function that performs a single step of the optimization\n",
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transition= memory.sample(BATCH_SIZE)\n",
    "    \n",
    "    '''\n",
    "    # Transpose the batch and convert the  batch-array of Experience\n",
    "    # to Transition of batch-arrays\n",
    "    '''\n",
    "    batch=Experience(*zip(*transition))\n",
    "    \n",
    "    #Compute a mask of non-final states and concatenate the batch element\n",
    "    \n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    \n",
    "    state_batch=torch.cat(batch.state)\n",
    "    action_batch=torch.cat(batch.action)\n",
    "    reward_batch=torch.cat(batch.reward)\n",
    "    \n",
    "    # Compute Q(s_t, a)\n",
    "    #These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_network(state_batch).gather(1,action_batch)\n",
    "    \n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    next_state_values = torch.zeros(BATCH_SIZE,device=device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_network(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = reward_batch + (GAMMA* next_state_values)\n",
    "    \n",
    "    # Compute Huber loss\n",
    "    criterion=nn.SmoothL1Loss()\n",
    "    loss=criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "    #optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_network.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1]\n",
      " [1 0 0 0 0 0 2 4 0 1]\n",
      " [1 0 0 0 2 0 2 0 0 1]\n",
      " [1 0 0 2 3 0 2 0 0 1]\n",
      " [1 0 0 0 2 2 2 0 0 1]\n",
      " [1 0 0 0 0 3 2 3 0 1]\n",
      " [1 0 2 2 2 0 2 2 0 1]\n",
      " [1 0 0 3 2 0 0 0 0 1]\n",
      " [1 0 0 0 2 0 0 0 0 1]\n",
      " [1 1 1 1 1 1 1 1 1 1]]\n",
      "{'dirt_cleaned': [0], 'agent_moved': [False], 'agent_charging': [False], 'agent_pos': [(7, 2)]}\n",
      "[(7, 2)]\n",
      "[[-1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1]]\n",
      "[[-1 -1 -1 -1 -1]\n",
      " [-1  0  2  2 -1]\n",
      " [ 1  0  0  3  2]\n",
      " [-1  0  0  0 -1]\n",
      " [-1 -1  1 -1 -1]]\n"
     ]
    }
   ],
   "source": [
    "#TODO: Get the agent_vision function working.\n",
    "\n",
    "env.reset()\n",
    "obs, info = env.get_observation()\n",
    "print(obs)\n",
    "print(info)\n",
    "\n",
    "agent_pos = info['agent_pos']\n",
    "\n",
    "g = Grid(obs.shape[0], obs.shape[1])\n",
    "g.cells = obs\n",
    "print(agent_pos)\n",
    "agent_view = agent_vision(loc=agent_pos[0], grid=g, vis_range=2)\n",
    "print(agent_view)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[1, 0, 0, 3, 2, 0, 0, 0, 0, 1],\n",
       "        [1, 0, 0, 0, 2, 0, 2, 0, 0, 1]]], dtype=int8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g.cells[agent_pos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]\n",
      " [-1 -1 -1 -1 -1 -1 -1]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "empty(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m agent_view \u001b[39m=\u001b[39m agent_vision(loc\u001b[39m=\u001b[39magent_pos[\u001b[39m0\u001b[39m], grid\u001b[39m=\u001b[39mg, vis_range\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m)\n\u001b[0;32m     11\u001b[0m input_shape \u001b[39m=\u001b[39m agent_view\u001b[39m.\u001b[39mshape\n\u001b[1;32m---> 13\u001b[0m policy_network \u001b[39m=\u001b[39m DQN(input_shape, n_actions, HIDDEN_SIZE)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     14\u001b[0m target_network \u001b[39m=\u001b[39m DQN(input_shape, n_actions, HIDDEN_SIZE)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     15\u001b[0m \u001b[39m#updates the parameters of the target network  with the parameters of the policy network \u001b[39;00m\n",
      "Cell \u001b[1;32mIn[7], line 13\u001b[0m, in \u001b[0;36mDQN.__init__\u001b[1;34m(self, n_observations, n_actions, hidden_size)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, n_observations, n_actions, hidden_size):\n\u001b[0;32m     12\u001b[0m     \u001b[39msuper\u001b[39m(DQN, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n\u001b[1;32m---> 13\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer1 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mLinear(n_observations, hidden_size)\n\u001b[0;32m     14\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer2 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear(hidden_size, hidden_size)\n\u001b[0;32m     15\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer3 \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mLinear( hidden_size, n_actions)\n",
      "File \u001b[1;32mc:\\Users\\20174216\\OneDrive - TU Eindhoven\\Documents\\1. TUe MSc DSAI\\Q4\\2AMC15 - Data Intelligence Challenge\\2AMC15-2023-DIC\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:96\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[1;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39min_features \u001b[39m=\u001b[39m in_features\n\u001b[0;32m     95\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_features \u001b[39m=\u001b[39m out_features\n\u001b[1;32m---> 96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweight \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty((out_features, in_features), \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n\u001b[0;32m     97\u001b[0m \u001b[39mif\u001b[39;00m bias:\n\u001b[0;32m     98\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbias \u001b[39m=\u001b[39m Parameter(torch\u001b[39m.\u001b[39mempty(out_features, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfactory_kwargs))\n",
      "\u001b[1;31mTypeError\u001b[0m: empty(): argument 'size' must be tuple of ints, but found element of type tuple at pos 2"
     ]
    }
   ],
   "source": [
    "#TODO: Change agent_vision code if needed.\n",
    "\n",
    "n_actions = 4\n",
    "vis_range = 3\n",
    "env.reset()\n",
    "obs, info = env.get_observation()\n",
    "agent_pos = info['agent_pos']\n",
    "g = Grid(obs.shape[0], obs.shape[1])\n",
    "g.cells = obs\n",
    "agent_view = agent_vision(loc=agent_pos[0], grid=g, vis_range=3)\n",
    "input_shape = agent_view.shape\n",
    "\n",
    "policy_network = DQN(input_shape, n_actions, HIDDEN_SIZE).to(device)\n",
    "target_network = DQN(input_shape, n_actions, HIDDEN_SIZE).to(device)\n",
    "#updates the parameters of the target network  with the parameters of the policy network \n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_network.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "steps_done = 0\n",
    "episode_duration = []\n",
    "num_episodes = 5\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    num_episodes = 600\n",
    "else:\n",
    "    num_episodes = 50\n",
    "\n",
    "for i in range(num_episodes):\n",
    "    #At the beginning we reset the environment and \n",
    "    #initialize the state Tensor.\n",
    "    env.reset()\n",
    "    obs, info = env.get_observation()\n",
    "    agent_pos = info['agent_pos']\n",
    "    g = Grid(obs.shape[0], obs.shape[1])\n",
    "    g.cells = obs\n",
    "    agent_view = agent_vision(loc=agent_pos[0], grid=g, vis_range=vis_range)\n",
    "    state = torch.tensor(agent_view, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "\n",
    "    for t in count():\n",
    "        # Sample an action\n",
    "        action = choose_action(state)\n",
    "        # Execute it, observe the next state and the reward\n",
    "        next_obs, reward, terminated, info = env.step([action])\n",
    "        next_agent_pos = info['agent_pos']\n",
    "        next_g = Grid(next_obs.shape[0], next_obs.shape[1])\n",
    "        next_g.cells = next_obs\n",
    "        next_agent_view = agent_vision(loc=next_agent_pos[0], grid=next_g, vis_range=vis_range)\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(next_agent_view, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "\n",
    "        # Store the experience in the memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state and grid\n",
    "        state = next_state\n",
    "        obs = next_obs\n",
    "        \n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        # The agent performs an optimization step on the Policy Network using the stored memory\n",
    "        optimize_model()\n",
    "\n",
    "        '''\n",
    "        The agent will perform a soft update of the Target Network's weights, \n",
    "        with the equation TAU * policy_net_state_dict + (1-TAU) * target_net_state_dict, \n",
    "        this helps to make the Target Network's weights converge to the Policy Network's weights.\n",
    "        '''\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        # policy_network.state_dict() returns the parameters of the policy network\n",
    "        # target_network.load_state_dict() loads these parameters into the target network.\n",
    "        target_net_state_dict = target_network.state_dict()\n",
    "        policy_net_state_dict = policy_network.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_network.load_state_dict(target_net_state_dict)\n",
    "        total_reward += reward\n",
    "\n",
    "        if terminated:\n",
    "            episode_duration.append(t + 1)\n",
    "            print(\"Episode\", i,\"Game terminated after\", t, \"steps with reward\", total_reward)\n",
    "            \n",
    "            break\n",
    "        \n",
    "print('Complete')\n",
    "plot_scores(show_result=True)\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
